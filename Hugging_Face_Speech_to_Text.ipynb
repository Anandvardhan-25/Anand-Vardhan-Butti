from transformers import pipeline
import transformers
print(transformers.__version__)
import librosa
import torch
import IPython.display as display
from transformers import Wav2Vec2ForCTC,Wav2Vec2Tokenizer
import numpy as np
tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
model1 = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
import librosa
audio, sampling_rate = librosa.load("/mmm.wav", sr=16000)
audio,sampling_rate
display.Audio('/mmm.wav',autoplay=True)
input_values = tokenizer(audio,return_tensors = 'pt').input_values
logits = model1(input_values).logits
predicted_ids = torch.argmax(logits,dim=-1)
transcriptions = tokenizer.decode(predicted_ids[0])
transcriptions explain me this code 
